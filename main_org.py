import numpy as np
from DDPG import DDPGAgent    
from aliengo_env import AliengoEnv
import matplotlib.pyplot as plt
import mujoco
from mujoco import viewer
import os
os.makedirs("saved_models", exist_ok=True)
"""
í•™ìŠµì„ ìœ„í•œ main.py ì½”ë“œ
"""

def main():
    # ===========================
    # 1) í™˜ê²½ ìƒì„±
    # ===========================
    env = AliengoEnv(model_path="aliengo/aliengo.xml")

    # ìƒíƒœ/í–‰ë™ ì°¨ì›
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    print("Environment created:")
    print(" - State dimension :", state_dim)
    print(" - Action dimension:", action_dim)

    # ===========================
    # 2) DDPG Agent ì´ˆê¸°í™”
    # ===========================
    agent = DDPGAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        gamma=0.99,
        tau=0.001,
        actor_lr=1e-4,
        critic_lr=1e-3,
        buffer_size=100000,
        batch_size=128,
        max_action=1.0,   # action_spaceê°€ [-1,1]ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    )
    print("DDPG Agent initialized.")

    # ==========================
    # Mujoco viewer ì‹¤í–‰
    # ==========================
    viewer = mujoco.viewer.launch_passive(env.model, env.data)
    print("MuJoCo Viewer launched.")

    # ===========================
    # 3) Episode Loop ê´€ë¦¬
    # ===========================
    max_episodes = 5000
    max_steps = env.max_episode_steps
    reward_history = []
    critic_loss_history = []
    actor_loss_history = []

    for episode in range(max_episodes):

        state = env.reset()
        agent.reset_noise()
        episode_reward = 0.0
        steps_in_episode = 0 

        print(f"===== Episode {episode} =====")

        for step in range(max_steps):

            if step < 10:
                action = np.zeros(action_dim)
            else:
                action = agent.act(state)

            # viewer ì¢…ë£Œ ë²„íŠ¼ ëˆ„ë¥´ë©´ í•™ìŠµ ì¤‘ë‹¨
            if not viewer.is_running():
                print("Viewer closed â€” training stopped.")
                viewer.close()
                return

            # --------------------------
            # í™˜ê²½ì—ì„œ step ìˆ˜í–‰
            # --------------------------
            next_state, reward, done, _ = env.step(action)

            # --------------------------
            # transition ì €ì¥ (Replay Buffer)
            # --------------------------
            agent.remember(state, action, reward, next_state, done)

            # --------------------------
            # DDPG í•™ìŠµ
            # --------------------------
            critic_loss, actor_loss = agent.train()

            if critic_loss is not None:
                critic_loss_history.append(critic_loss)
                actor_loss_history.append(actor_loss)

            # --------------------------
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            # --------------------------
            state = next_state
            episode_reward += reward
            state = next_state
            steps_in_episode += 1 

            # ===========================
            # Viewer ì—…ë°ì´íŠ¸
            # ===========================
            viewer.sync()

            # --------------------------
            # ì¢…ë£Œ ì¡°ê±´
            # --------------------------
            if done:
                break


        reward_history.append(episode_reward)


        print(
            f"[Episode {episode}] Reward: {episode_reward:.2f}, "
            f"Steps: {steps_in_episode}, "
            f"CriticLoss: {critic_loss}, ActorLoss: {actor_loss}"
        )
        print("[INFO] Saving model...")
        agent.save(
            actor_path="saved_models/actor.weights.h5",
            critic_path="saved_models/critic.weights.h5"
        )

        print("[INFO] Model saved.")


    # ===========================
    # Viewer ì¢…ë£Œ
    # ===========================
    viewer.close()

    # --------------------------
    # í•™ìŠµ ë¦¬ì›Œë“œ ì €ì¥
    # --------------------------
    np.save("reward_history.npy", reward_history)

    # ==================================================
    # 4) í•™ìŠµ ê²°ê³¼ Plot
    # ==================================================

    # ----- Reward -----
    plt.figure(figsize=(10,5))
    plt.plot(reward_history, label="Episode Reward")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("Reward per Episode")
    plt.grid(True)
    plt.legend()
    plt.savefig("reward_plot.png")   # ğŸ”¥ ì €ì¥
    plt.close()

    # ----- Critic Loss -----
    plt.figure(figsize=(10,5))
    plt.plot(critic_loss_history, label="Critic Loss", color='r')
    plt.xlabel("Training Step")
    plt.ylabel("Loss")
    plt.title("Critic Loss over Training")
    plt.grid(True)
    plt.legend()
    plt.savefig("critic_loss_plot.png")   # ğŸ”¥ ì €ì¥
    plt.close()

    # ----- Actor Loss -----
    plt.figure(figsize=(10,5))
    plt.plot(actor_loss_history, label="Actor Loss", color='g')
    plt.xlabel("Training Step")
    plt.ylabel("Loss")
    plt.title("Actor Loss over Training")
    plt.grid(True)
    plt.legend()
    plt.savefig("actor_loss_plot.png")   # ğŸ”¥ ì €ì¥
    plt.close()

    

if __name__ == "__main__":
    main()
